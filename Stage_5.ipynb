{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a5004b2-ffa9-48b8-8303-6b4fb0db76d8",
   "metadata": {},
   "source": [
    "## ‚úçÔ∏è Stage 5: Sequence Models and Natural Language Processing (NLP)\n",
    "\n",
    "### Topics:\n",
    "- Recurrent Neural Networks (RNNs)\n",
    "- Encoder-decoder architectures\n",
    "- LSTMs, Seq2Seq\n",
    "- Attention Mechanisms and Multi-Head Attention.\n",
    "- Transformers: Architecture, BERT, GPT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e82af09-14f5-4e6e-aec3-29ce0e0acc30",
   "metadata": {},
   "source": [
    "## üîÅ Recurrent Neural Networks (RNNs)\n",
    "\n",
    "Recurrent Neural Networks are designed to work with **sequential data** such as text, time series, or speech. Unlike traditional neural networks that treat each input independently, RNNs maintain a **memory** of past inputs through a hidden state that gets updated over time.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† **Intuitive Explanation**\n",
    "\n",
    "Imagine you're reading a sentence word by word. Your understanding of each new word depends on the words you've already read. RNNs work the same way ‚Äî they **process one input at a time**, updating an internal state to \"remember\" past information.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è **Technical Explanation**\n",
    "\n",
    "At each time step $t$, an RNN takes:\n",
    "- an input vector $x_t$\n",
    "- the previous hidden state $h_{t-1}$\n",
    "\n",
    "It computes the new hidden state as:\n",
    "\n",
    "$$\n",
    "h_t = \\tanh(W_{xh} x_t + W_{hh} h_{t-1} + b_h)\n",
    "$$\n",
    "\n",
    "And optionally, the output:\n",
    "\n",
    "$$\n",
    "y_t = W_{hy} h_t + b_y\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $W_{xh}$ is the input-to-hidden weights\n",
    "- $W_{hh}$ is the hidden-to-hidden (recurrent) weights\n",
    "- $W_{hy}$ is the hidden-to-output weights\n",
    "- $b_h$, $b_y$ are bias terms\n",
    "- $\\tanh$ is the activation function (other options like ReLU are also used)\n",
    "\n",
    "---\n",
    "\n",
    "### üí° **Core Ideas**\n",
    "\n",
    "- **Weight Sharing**: The same weights are used at each time step.\n",
    "- **Memory**: The hidden state $h_t$ summarizes information from all previous inputs $(x_1, x_2, ..., x_t)$.\n",
    "- **Backpropagation Through Time (BPTT)** is used to train the model by unrolling it over time steps.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è **Challenges**\n",
    "\n",
    "- üîΩ **Vanishing Gradients**: Difficult to learn long-range dependencies.\n",
    "- üîº **Exploding Gradients**: Can lead to unstable training.\n",
    "\n",
    "---\n",
    "\n",
    "### üìå **When to Use RNNs**\n",
    "\n",
    "Use RNNs when:\n",
    "- The **order** of inputs matters (e.g., sentences, time series).\n",
    "- You need to model **temporal dependencies** or **context**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8173e53d-27ab-4370-97bf-b708c94b21b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.2.1 (20241206.2353)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"530pt\" height=\"244pt\"\n",
       " viewBox=\"0.00 0.00 530.07 244.26\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 240.26)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-240.26 526.07,-240.26 526.07,4 -4,4\"/>\n",
       "<!-- x0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>x0</title>\n",
       "<ellipse fill=\"lightblue\" stroke=\"black\" cx=\"20.13\" cy=\"-205.13\" rx=\"20.13\" ry=\"20.13\"/>\n",
       "<text text-anchor=\"middle\" x=\"20.13\" y=\"-200.08\" font-family=\"Times,serif\" font-size=\"14.00\">x0</text>\n",
       "</g>\n",
       "<!-- h0 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>h0</title>\n",
       "<ellipse fill=\"lightgreen\" stroke=\"black\" cx=\"116.64\" cy=\"-205.13\" rx=\"20.13\" ry=\"20.13\"/>\n",
       "<text text-anchor=\"middle\" x=\"116.64\" y=\"-200.08\" font-family=\"Times,serif\" font-size=\"14.00\">h0</text>\n",
       "</g>\n",
       "<!-- x0&#45;&gt;h0 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>x0&#45;&gt;h0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M40.71,-205.13C53.39,-205.13 70.16,-205.13 84.62,-205.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"84.57,-208.63 94.57,-205.13 84.57,-201.63 84.57,-208.63\"/>\n",
       "<text text-anchor=\"middle\" x=\"68.39\" y=\"-208.33\" font-family=\"Times,serif\" font-size=\"14.00\">Wx</text>\n",
       "</g>\n",
       "<!-- h1 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>h1</title>\n",
       "<ellipse fill=\"lightgreen\" stroke=\"black\" cx=\"213.16\" cy=\"-147.13\" rx=\"20.13\" ry=\"20.13\"/>\n",
       "<text text-anchor=\"middle\" x=\"213.16\" y=\"-142.08\" font-family=\"Times,serif\" font-size=\"14.00\">h1</text>\n",
       "</g>\n",
       "<!-- h0&#45;&gt;h1 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>h0&#45;&gt;h1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M134.11,-195.03C148.38,-186.27 169.3,-173.43 185.89,-163.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"187.58,-166.32 194.27,-158.1 183.92,-160.35 187.58,-166.32\"/>\n",
       "<text text-anchor=\"middle\" x=\"164.9\" y=\"-184.53\" font-family=\"Times,serif\" font-size=\"14.00\">Wh</text>\n",
       "</g>\n",
       "<!-- y0 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>y0</title>\n",
       "<ellipse fill=\"lightcoral\" stroke=\"black\" cx=\"213.16\" cy=\"-216.13\" rx=\"20.13\" ry=\"20.13\"/>\n",
       "<text text-anchor=\"middle\" x=\"213.16\" y=\"-211.08\" font-family=\"Times,serif\" font-size=\"14.00\">y0</text>\n",
       "</g>\n",
       "<!-- h0&#45;&gt;y0 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>h0&#45;&gt;y0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M136.77,-207.36C149.62,-208.85 166.84,-210.86 181.58,-212.57\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"180.89,-216.02 191.23,-213.7 181.7,-209.06 180.89,-216.02\"/>\n",
       "<text text-anchor=\"middle\" x=\"164.9\" y=\"-214.82\" font-family=\"Times,serif\" font-size=\"14.00\">Wy</text>\n",
       "</g>\n",
       "<!-- x1 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>x1</title>\n",
       "<ellipse fill=\"lightblue\" stroke=\"black\" cx=\"116.64\" cy=\"-136.13\" rx=\"20.13\" ry=\"20.13\"/>\n",
       "<text text-anchor=\"middle\" x=\"116.64\" y=\"-131.08\" font-family=\"Times,serif\" font-size=\"14.00\">x1</text>\n",
       "</g>\n",
       "<!-- x1&#45;&gt;h1 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>x1&#45;&gt;h1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M137.12,-135.56C148.27,-135.5 162.49,-135.89 175.02,-137.63 177.39,-137.96 179.83,-138.39 182.26,-138.87\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"181.42,-142.27 191.96,-141.13 183.01,-135.45 181.42,-142.27\"/>\n",
       "<text text-anchor=\"middle\" x=\"164.9\" y=\"-140.83\" font-family=\"Times,serif\" font-size=\"14.00\">Wx</text>\n",
       "</g>\n",
       "<!-- h2 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>h2</title>\n",
       "<ellipse fill=\"lightgreen\" stroke=\"black\" cx=\"309.67\" cy=\"-89.13\" rx=\"20.13\" ry=\"20.13\"/>\n",
       "<text text-anchor=\"middle\" x=\"309.67\" y=\"-84.08\" font-family=\"Times,serif\" font-size=\"14.00\">h2</text>\n",
       "</g>\n",
       "<!-- h1&#45;&gt;h2 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>h1&#45;&gt;h2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M230.62,-137.03C244.9,-128.27 265.82,-115.43 282.41,-105.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"284.09,-108.32 290.79,-100.1 280.43,-102.35 284.09,-108.32\"/>\n",
       "<text text-anchor=\"middle\" x=\"261.41\" y=\"-126.53\" font-family=\"Times,serif\" font-size=\"14.00\">Wh</text>\n",
       "</g>\n",
       "<!-- y1 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>y1</title>\n",
       "<ellipse fill=\"lightcoral\" stroke=\"black\" cx=\"309.67\" cy=\"-158.13\" rx=\"20.13\" ry=\"20.13\"/>\n",
       "<text text-anchor=\"middle\" x=\"309.67\" y=\"-153.08\" font-family=\"Times,serif\" font-size=\"14.00\">y1</text>\n",
       "</g>\n",
       "<!-- h1&#45;&gt;y1 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>h1&#45;&gt;y1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M233.28,-149.36C246.14,-150.85 263.35,-152.86 278.09,-154.57\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"277.4,-158.02 287.74,-155.7 278.21,-151.06 277.4,-158.02\"/>\n",
       "<text text-anchor=\"middle\" x=\"261.41\" y=\"-156.82\" font-family=\"Times,serif\" font-size=\"14.00\">Wy</text>\n",
       "</g>\n",
       "<!-- x2 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>x2</title>\n",
       "<ellipse fill=\"lightblue\" stroke=\"black\" cx=\"213.16\" cy=\"-78.13\" rx=\"20.13\" ry=\"20.13\"/>\n",
       "<text text-anchor=\"middle\" x=\"213.16\" y=\"-73.08\" font-family=\"Times,serif\" font-size=\"14.00\">x2</text>\n",
       "</g>\n",
       "<!-- x2&#45;&gt;h2 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>x2&#45;&gt;h2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M233.63,-77.56C244.78,-77.5 259,-77.89 271.54,-79.63 273.91,-79.96 276.34,-80.39 278.78,-80.87\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"277.93,-84.27 288.47,-83.13 279.52,-77.45 277.93,-84.27\"/>\n",
       "<text text-anchor=\"middle\" x=\"261.41\" y=\"-82.83\" font-family=\"Times,serif\" font-size=\"14.00\">Wx</text>\n",
       "</g>\n",
       "<!-- h3 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>h3</title>\n",
       "<ellipse fill=\"lightgreen\" stroke=\"black\" cx=\"406.18\" cy=\"-31.13\" rx=\"20.13\" ry=\"20.13\"/>\n",
       "<text text-anchor=\"middle\" x=\"406.18\" y=\"-26.08\" font-family=\"Times,serif\" font-size=\"14.00\">h3</text>\n",
       "</g>\n",
       "<!-- h2&#45;&gt;h3 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>h2&#45;&gt;h3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M327.13,-79.03C341.41,-70.27 362.33,-57.43 378.92,-47.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"380.61,-50.32 387.3,-42.1 376.95,-44.35 380.61,-50.32\"/>\n",
       "<text text-anchor=\"middle\" x=\"357.92\" y=\"-68.53\" font-family=\"Times,serif\" font-size=\"14.00\">Wh</text>\n",
       "</g>\n",
       "<!-- y2 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>y2</title>\n",
       "<ellipse fill=\"lightcoral\" stroke=\"black\" cx=\"406.18\" cy=\"-100.13\" rx=\"20.13\" ry=\"20.13\"/>\n",
       "<text text-anchor=\"middle\" x=\"406.18\" y=\"-95.08\" font-family=\"Times,serif\" font-size=\"14.00\">y2</text>\n",
       "</g>\n",
       "<!-- h2&#45;&gt;y2 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>h2&#45;&gt;y2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M329.79,-91.36C342.65,-92.85 359.87,-94.86 374.6,-96.57\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"373.92,-100.02 384.25,-97.7 374.73,-93.06 373.92,-100.02\"/>\n",
       "<text text-anchor=\"middle\" x=\"357.92\" y=\"-98.82\" font-family=\"Times,serif\" font-size=\"14.00\">Wy</text>\n",
       "</g>\n",
       "<!-- x3 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>x3</title>\n",
       "<ellipse fill=\"lightblue\" stroke=\"black\" cx=\"309.67\" cy=\"-20.13\" rx=\"20.13\" ry=\"20.13\"/>\n",
       "<text text-anchor=\"middle\" x=\"309.67\" y=\"-15.08\" font-family=\"Times,serif\" font-size=\"14.00\">x3</text>\n",
       "</g>\n",
       "<!-- x3&#45;&gt;h3 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>x3&#45;&gt;h3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M330.15,-19.56C341.29,-19.5 355.51,-19.89 368.05,-21.63 370.42,-21.96 372.85,-22.39 375.29,-22.87\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"374.45,-26.27 384.98,-25.13 376.03,-19.45 374.45,-26.27\"/>\n",
       "<text text-anchor=\"middle\" x=\"357.92\" y=\"-24.83\" font-family=\"Times,serif\" font-size=\"14.00\">Wx</text>\n",
       "</g>\n",
       "<!-- y3 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>y3</title>\n",
       "<ellipse fill=\"lightcoral\" stroke=\"black\" cx=\"501.94\" cy=\"-31.13\" rx=\"20.13\" ry=\"20.13\"/>\n",
       "<text text-anchor=\"middle\" x=\"501.94\" y=\"-26.08\" font-family=\"Times,serif\" font-size=\"14.00\">y3</text>\n",
       "</g>\n",
       "<!-- h3&#45;&gt;y3 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>h3&#45;&gt;y3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M426.61,-31.13C439.18,-31.13 455.82,-31.13 470.17,-31.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"470.03,-34.63 480.03,-31.13 470.03,-27.63 470.03,-34.63\"/>\n",
       "<text text-anchor=\"middle\" x=\"454.06\" y=\"-34.33\" font-family=\"Times,serif\" font-size=\"14.00\">Wy</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x10650e560>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def draw_rnn_unrolled(time_steps=3):\n",
    "    dot = Digraph(format='png')\n",
    "    dot.attr(rankdir='LR')  # Left to right\n",
    "\n",
    "    # Add nodes for hidden states and inputs\n",
    "    for t in range(time_steps):\n",
    "        dot.node(f\"x{t}\", f\"x{t}\", shape=\"circle\", style=\"filled\", fillcolor=\"lightblue\")\n",
    "        dot.node(f\"h{t}\", f\"h{t}\", shape=\"circle\", style=\"filled\", fillcolor=\"lightgreen\")\n",
    "\n",
    "    # Add edges for input to hidden connections\n",
    "    for t in range(time_steps):\n",
    "        dot.edge(f\"x{t}\", f\"h{t}\", label=\"Wx\")\n",
    "\n",
    "    # Add recurrent connections\n",
    "    for t in range(1, time_steps):\n",
    "        dot.edge(f\"h{t-1}\", f\"h{t}\", label=\"Wh\")\n",
    "\n",
    "    # Add output nodes (optional)\n",
    "    for t in range(time_steps):\n",
    "        dot.node(f\"y{t}\", f\"y{t}\", shape=\"circle\", style=\"filled\", fillcolor=\"lightcoral\")\n",
    "        dot.edge(f\"h{t}\", f\"y{t}\", label=\"Wy\")\n",
    "\n",
    "    return dot\n",
    "\n",
    "# Display the graph\n",
    "draw_rnn_unrolled(time_steps=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09a17edb-7547-47af-b303-dc41e15abe57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, Loss: 2825.6826\n",
      "Epoch 100, Loss: 2380.5339\n",
      "Epoch 150, Loss: 2006.9341\n",
      "Epoch 200, Loss: 1693.4469\n",
      "Epoch 250, Loss: 1430.2433\n",
      "Epoch 300, Loss: 1209.1809\n",
      "\n",
      "Prediction for input [10, 11, 12]: 13.01\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Dummy data: sequence of numbers from 0 to 1000 ‚Üí learn to predict the next number\n",
    "sequence = np.array([i for i in range(100)], dtype=np.float32)\n",
    "\n",
    "# Prepare sequences: input (x) and target (y)\n",
    "def create_dataset(seq, input_size):\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(len(seq) - input_size):\n",
    "        x.append(seq[i:i+input_size])\n",
    "        y.append(seq[i+input_size])\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "input_size = 3  # how many steps to look back\n",
    "x, y = create_dataset(sequence, input_size)\n",
    "\n",
    "# Convert to tensors\n",
    "x_tensor = torch.tensor(x).unsqueeze(-1)  # shape (batch, seq_len, 1)\n",
    "y_tensor = torch.tensor(y).unsqueeze(-1)  # shape (batch, 1)\n",
    "\n",
    "# Define the RNN model\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size=input_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        last_out = out[:, -1, :]  # take last hidden state\n",
    "        return self.fc(last_out)\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = 1 #Number of features\n",
    "hidden_dim = 10 #number of \"neurons\"\n",
    "output_dim = 1\n",
    "lr = 0.01\n",
    "epochs = 300\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = SimpleRNN(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x_tensor)\n",
    "    loss = criterion(output, y_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Test prediction\n",
    "model.eval()\n",
    "test_input = torch.tensor([[10, 11, 12]], dtype=torch.float32).unsqueeze(-1)  # (1, 3, 1)\n",
    "pred = model(test_input).item()\n",
    "print(f\"\\nPrediction for input [10, 11, 12]: {pred:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97f8c32-2acf-4e2d-b49f-aa7748d94a4d",
   "metadata": {},
   "source": [
    "## üèóÔ∏è General Encoder-Decoder Framework\n",
    "\n",
    "- The **encoder** compresses the input (which could be a sequence, an image, etc.) into a **context vector** (or set of vectors).\n",
    "- The **decoder** uses this context to **generate the output**.\n",
    "\n",
    "The **types of encoders and decoders** can vary depending on the task:\n",
    "\n",
    "### üî∂ Types of Encoder-Decoder Models\n",
    "\n",
    "| Type                    | Encoder                  | Decoder                         | Example Task                                    |\n",
    "|-------------------------|--------------------------|----------------------------------|-------------------------------------------------|\n",
    "| **RNN-based**           | RNN/LSTM/GRU             | RNN/LSTM/GRU                    | Machine translation, text summarization         |\n",
    "| **CNN-based**           | CNN (Convolutional)      | Deconvolutional CNN (transposed conv) | Image segmentation, super-resolution   |\n",
    "| **ANN (Feedforward)**   | Fully connected layers   | Fully connected layers          | Autoencoders for dimensionality reduction       |\n",
    "| **Transformer-based**   | Self-attention layers    | Self-attention layers           | Text translation, summarization                |\n",
    "| **Variational Autoencoders (VAE)** | ANN/Conv + stochastic sampling | ANN/Conv        | Generative modeling                        |\n",
    "\n",
    "---\n",
    "\n",
    "## üèôÔ∏è Encoder-Decoder with ANNs\n",
    "\n",
    "In fact, **autoencoders** are a classic example of an encoder-decoder using **only feedforward layers (ANNs)**.\n",
    "\n",
    "- üîπ **Encoder:** Compresses the input into a latent code (vector).\n",
    "- üîπ **Decoder:** Reconstructs the input from the latent code.\n",
    "\n",
    "Example:\n",
    "- Input: `[x1, x2, x3]`\n",
    "- Encoder: Dense layers ‚Üí latent representation (e.g., `[z1, z2]`)\n",
    "- Decoder: Dense layers ‚Üí reconstructed `[x1', x2', x3']`\n",
    "\n",
    "---\n",
    "\n",
    "## ü§î So why are RNNs often used?\n",
    "\n",
    "- For **sequence data**, RNNs (or their modern replacements like **Transformers**) naturally handle variable-length input.\n",
    "- **ANNs** (feedforward networks) are **fixed-size** input-output models, so they‚Äôre great for autoencoders and other fixed-dimension tasks (e.g., image compression) but less ideal for sequences.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f280d3e2-e37c-46b1-8ee7-df2db1390162",
   "metadata": {},
   "source": [
    "# üß† What is an LSTM?\n",
    "\n",
    "**LSTM** stands for **Long Short-Term Memory**, and it‚Äôs a special type of **Recurrent Neural Network (RNN)**.  \n",
    "It was designed to solve the **vanishing gradient problem** that standard RNNs face when learning long-term dependencies.\n",
    "\n",
    "- **RNNs** process data **step-by-step** in a sequence, passing information (hidden state) from one time step to the next.\n",
    "- However, standard RNNs struggle to remember information over long sequences because the gradients used for learning tend to either **vanish** (become too small) or **explode** (become too large).\n",
    "\n",
    "LSTMs fix this by using a **memory cell** and **gates** to control the flow of information.\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è LSTM Cell Structure\n",
    "\n",
    "An **LSTM cell** contains:\n",
    "- **Cell state ($C$)**: The memory of the cell.\n",
    "- **Hidden state ($h$)**: The output at the current time step.\n",
    "- **Input ($x_t$)**: The input at time step $t$.\n",
    "\n",
    "It also has **3 gates**:\n",
    "1Ô∏è‚É£ **Forget Gate**: Decides what information to forget from the cell state.  \n",
    "2Ô∏è‚É£ **Input Gate**: Decides what new information to add to the cell state.  \n",
    "3Ô∏è‚É£ **Output Gate**: Decides what part of the cell state to output.\n",
    "\n",
    "---\n",
    "\n",
    "## üî¢ Basic Equations\n",
    "\n",
    "At time step $t$:\n",
    "\n",
    "- **Forget gate:**  \n",
    "$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$\n",
    "\n",
    "- **Input gate:**  \n",
    "$i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$\n",
    "\n",
    "- **Candidate cell state:**  \n",
    "$\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$\n",
    "\n",
    "- **Cell state update:**  \n",
    "$C_t = f_t * C_{t-1} + i_t * \\tilde{C}_t$\n",
    "\n",
    "- **Output gate:**  \n",
    "$o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$\n",
    "\n",
    "- **Hidden state:**  \n",
    "$h_t = o_t * \\tanh(C_t)$\n",
    "\n",
    "---\n",
    "\n",
    "## üåä What Does It All Mean?\n",
    "\n",
    "‚úÖ **Forget gate**: Keeps or discards old memory.  \n",
    "‚úÖ **Input gate**: Adds new memory.  \n",
    "‚úÖ **Output gate**: Determines what to output.\n",
    "\n",
    "This allows LSTMs to **\"remember\" important information for long sequences** and **forget unimportant parts**.\n",
    "\n",
    "---\n",
    "\n",
    "## ü§î Why LSTMs are Powerful\n",
    "\n",
    "- RNNs can forget long-term dependencies.  \n",
    "- LSTMs **solve this**, making them great for tasks like:\n",
    "  - Language modeling\n",
    "  - Text generation\n",
    "  - Time-series prediction\n",
    "  - Speech recognition\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf84d966-1dd9-44e1-9be4-8464cffef171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, Loss: 2783.0605\n",
      "Epoch 100, Loss: 2321.4629\n",
      "Epoch 150, Loss: 1947.4984\n",
      "Epoch 200, Loss: 1578.0094\n",
      "Epoch 250, Loss: 1302.0115\n",
      "Epoch 300, Loss: 1082.3044\n",
      "\n",
      "Prediction for input [10, 11, 12]: 13.14\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Dummy data: sequence of numbers from 0 to 1000 ‚Üí learn to predict the next number\n",
    "sequence = np.array([i for i in range(100)], dtype=np.float32)\n",
    "\n",
    "# Prepare sequences: input (x) and target (y)\n",
    "def create_dataset(seq, input_size):\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(len(seq) - input_size):\n",
    "        x.append(seq[i:i+input_size])\n",
    "        y.append(seq[i+input_size])\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "input_size = 3  # how many steps to look back\n",
    "x, y = create_dataset(sequence, input_size)\n",
    "\n",
    "# Convert to tensors\n",
    "x_tensor = torch.tensor(x).unsqueeze(-1)  # shape (batch, seq_len, 1)\n",
    "y_tensor = torch.tensor(y).unsqueeze(-1)  # shape (batch, 1)\n",
    "\n",
    "# Define the RNN model\n",
    "class LSTMS(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LSTMS, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        _, (hn, _) = self.lstm(x)  # hn: (num_layers, batch, hidden_dim)\n",
    "        out = self.fc(hn[-1])      # Take last layer's hidden state\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = 1 #Number of features\n",
    "hidden_dim = 10 #number of \"neurons\"\n",
    "output_dim = 1\n",
    "lr = 0.01\n",
    "epochs = 300\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = LSTMS(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x_tensor)\n",
    "    loss = criterion(output, y_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Test prediction\n",
    "model.eval()\n",
    "test_input = torch.tensor([[10, 11, 12]], dtype=torch.float32).unsqueeze(-1)  # (1, 3, 1)\n",
    "pred = model(test_input).item()\n",
    "print(f\"\\nPrediction for input [10, 11, 12]: {pred:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7d3943-43e5-4a6c-bb27-45409abc3be7",
   "metadata": {},
   "source": [
    "# üî• Seq2Seq (Sequence-to-Sequence) Models üî•\n",
    "\n",
    "A **Seq2Seq** model is a **neural architecture** used to **map an input sequence to an output sequence**. It consists of:\n",
    "\n",
    "- üèóÔ∏è **Encoder**: Transforms the input sequence into a **fixed-size latent vector (context)**.  \n",
    "- üèóÔ∏è **Decoder**: Uses the context to generate the output sequence **step by step**.  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Mathematical Formulation\n",
    "\n",
    "Given input sequence **$X = (x_1, x_2, \\dots, x_T)$** and output sequence **$Y = (y_1, y_2, \\dots, y_{T'})$**:  \n",
    "\n",
    "- The **encoder** computes hidden states **$h_t$**:  \n",
    "$$ h_t = f_{\\text{enc}}(x_t, h_{t-1}) $$\n",
    "- The **context vector** $c$ summarizes the sequence:  \n",
    "$$ c = h_T $$\n",
    "- The **decoder** generates each output token conditioned on $c$ and past outputs:  \n",
    "$$ s_t = f_{\\text{dec}}(y_{t-1}, s_{t-1}, c) $$\n",
    "$$ \\hat{y}_t = g(s_t) $$\n",
    "\n",
    "---\n",
    "\n",
    "## üìà Use Cases\n",
    "- üó£Ô∏è **Machine translation** (e.g., English ‚Üí Spanish)  \n",
    "- üìù **Text summarization**  \n",
    "- ü§ñ **Chatbots and dialogue generation**  \n",
    "- üìâ **Time series forecasting** (encoder-decoder for multistep prediction)  \n",
    "\n",
    "---\n",
    "\n",
    "## üï∏Ô∏è Key Concepts\n",
    "- üîí **Fixed-length bottleneck**: Vanilla Seq2Seq compresses the input into a single vector $c$ (limitation for long sequences).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74378207-a861-4f60-a79f-87354caeb517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.5810\n",
      "Epoch 2, Loss: 0.8332\n",
      "Epoch 3, Loss: 0.3937\n",
      "Epoch 4, Loss: 0.2235\n",
      "Epoch 5, Loss: 0.1468\n",
      "Epoch 6, Loss: 0.1018\n",
      "Epoch 7, Loss: 0.0732\n",
      "Epoch 8, Loss: 0.0573\n",
      "Epoch 9, Loss: 0.0431\n",
      "Epoch 10, Loss: 0.0311\n",
      "Epoch 11, Loss: 0.0279\n",
      "Epoch 12, Loss: 0.0220\n",
      "Epoch 13, Loss: 0.0173\n",
      "Epoch 14, Loss: 0.0196\n",
      "Epoch 15, Loss: 0.0135\n",
      "Epoch 16, Loss: 0.0142\n",
      "Epoch 17, Loss: 0.0088\n",
      "Epoch 18, Loss: 0.0128\n",
      "Epoch 19, Loss: 0.0090\n",
      "Epoch 20, Loss: 0.0073\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from random import randint\n",
    "\n",
    "# üìå Hyperparameters\n",
    "INPUT_DIM = 12   # 0-9 digits + SOS + EOS\n",
    "OUTPUT_DIM = 12\n",
    "HID_DIM = 64\n",
    "N_EPOCHS = 20\n",
    "BATCH_SIZE = 64   # Bigger batch\n",
    "LEARNING_RATE = 0.001\n",
    "MAX_LEN = 7\n",
    "\n",
    "PAD, SOS, EOS = 0, 10, 11\n",
    "\n",
    "# üìö Encoder with LSTM\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        self.lstm = nn.LSTM(hid_dim, hid_dim)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return hidden, cell\n",
    "\n",
    "# üìö Decoder with LSTM\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hid_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.lstm = nn.LSTM(hid_dim, hid_dim)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        embedded = self.embedding(input).unsqueeze(0)\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "# üî• Seq2Seq Model\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_sixze = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        output_dim = self.decoder.fc_out.out_features\n",
    "        \n",
    "        outputs = torch.zeros(trg_len, batch_size, output_dim)\n",
    "        hidden, cell = self.encoder(src)\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[t] = output\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[t] if torch.rand(1).item() < teacher_forcing_ratio else top1\n",
    "        return outputs\n",
    "\n",
    "# ‚öôÔ∏è Initialize Model\n",
    "encoder = Encoder(INPUT_DIM, HID_DIM)\n",
    "decoder = Decoder(OUTPUT_DIM, HID_DIM)\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD)\n",
    "\n",
    "# üèóÔ∏è Bigger Synthetic Dataset\n",
    "def generate_example():\n",
    "    length = randint(3, MAX_LEN)\n",
    "    src_seq = [randint(1,9) for _ in range(length)]\n",
    "    trg_seq = src_seq[::-1]  # Reverse as target\n",
    "    src = [SOS] + src_seq + [EOS] + [PAD]*(MAX_LEN+2-len(src_seq)-2)\n",
    "    trg = [SOS] + trg_seq + [EOS] + [PAD]*(MAX_LEN+2-len(trg_seq)-2)\n",
    "    return torch.tensor(src), torch.tensor(trg)\n",
    "\n",
    "def generate_batch(batch_size):\n",
    "    src_batch, trg_batch = [], []\n",
    "    for _ in range(batch_size):\n",
    "        src, trg = generate_example()\n",
    "        src_batch.append(src.unsqueeze(1))\n",
    "        trg_batch.append(trg.unsqueeze(1))\n",
    "    src = torch.cat(src_batch, dim=1)\n",
    "    trg = torch.cat(trg_batch, dim=1)\n",
    "    return src, trg\n",
    "\n",
    "# üî• Training Loop\n",
    "for epoch in range(N_EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for _ in range(200):  # More batches per epoch\n",
    "        src, trg = generate_batch(BATCH_SIZE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].reshape(-1, output_dim)\n",
    "        trg_gold = trg[1:].reshape(-1)\n",
    "        \n",
    "        loss = criterion(output, trg_gold)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}, Loss: {epoch_loss/200:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdf2f4d-4836-4bf8-880b-14f601ddfc4c",
   "metadata": {},
   "source": [
    "# üîç Understanding the Attention Mechanism\n",
    "\n",
    "The **attention mechanism** is a powerful concept in deep learning, especially in models that process sequences, like in **natural language processing (NLP)** and **time series forecasting**.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Core Idea\n",
    "\n",
    "Traditional models like RNNs or LSTMs compress an entire input sequence into a single vector, which can make it difficult to retain information from earlier steps in long sequences.\n",
    "\n",
    "**Attention** allows the model to **focus on the most relevant parts of the input** when producing each output. Rather than treating all input positions equally, attention **assigns a weight** to each input element based on its relevance to the current output step.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Why It‚Äôs Useful\n",
    "\n",
    "- It reduces the burden of remembering long sequences.\n",
    "- It dynamically identifies which parts of the input are most useful at each output step.\n",
    "- It enables parallel processing (in Transformers) and improves performance on tasks like translation, summarization, and question answering.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è How Attention Works\n",
    "\n",
    "Let‚Äôs define:\n",
    "\n",
    "- **Query (Q)**: the decoder hidden state at the current time step (a vector)\n",
    "- **Key (K)**: the encoder hidden states for each time step in the input sequence (vectors)\n",
    "- **Value (V)**: the same as Keys, used to compute the final output.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. **Calculate raw attention scores**:\n",
    "\n",
    "$$\n",
    "\\text{score}(Q, K) = Q \\cdot K^T\n",
    "$$\n",
    "\n",
    "2. **Normalize the scores using softmax** to get attention weights:\n",
    "\n",
    "$$\n",
    "\\alpha_i = \\frac{e^{\\text{score}_i}}{\\sum_j e^{\\text{score}_j}}\n",
    "$$\n",
    "\n",
    "3. **Use the weights to compute a weighted average of the values**:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\sum_i \\alpha_i \\cdot V_i\n",
    "$$\n",
    "\n",
    "This gives a **context vector** summarizing the relevant parts of the input for the current step.\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Analogy\n",
    "\n",
    "Imagine writing a summary of a book. For each sentence you write, you might go back and re-read parts of the book. You don‚Äôt re-read everything‚Äîyou **focus on the important parts**. That‚Äôs attention!\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454862f7-9333-49cc-b443-bae3d0adeb3e",
   "metadata": {},
   "source": [
    "## üîç Comparison of Context Vectors in Sequence Models\n",
    "\n",
    "| **Model**              | **Context Vector** Description                                                                 | **Static/Dynamic** | **How It's Computed**                                                                                         | **Strengths**                                              | **Limitations**                                                |\n",
    "|------------------------|-----------------------------------------------------------------------------------------------|---------------------|----------------------------------------------------------------------------------------------------------------|------------------------------------------------------------|----------------------------------------------------------------|\n",
    "| **Vanilla RNN**        | Last hidden state of the encoder: $$ h_T $$                                                  | Static              | $$ h_t = \\tanh(W_h h_{t-1} + W_x x_t + b) $$                             | Simple implementation                                       | Struggles with long-term dependencies (vanishing gradients)     |\n",
    "| **LSTM**               | Last hidden state of the encoder: $$ h_T $$                                                  | Static              | Uses gates: $$ h_t = o_t \\odot \\tanh(c_t) $$, with memory cell $$ c_t $$ updated via input/forget gates       | Remembers longer sequences than RNN                        | Still compresses full sequence into one vector                  |\n",
    "| **Seq2Seq (RNN/LSTM)** | Uses encoder‚Äôs final hidden state as a context vector                                         | Static              | Same as above (RNN or LSTM depending on encoder architecture)                                                 | Effective for short sequences                              | Information loss for long sequences                             |\n",
    "| **With Attention**     | Weighted sum of all encoder hidden states: $$ c_t = \\sum_{i=1}^{T} \\alpha_{t,i} \\cdot h_i $$ | **Dynamic**         | Attention weights $$ \\alpha_{t,i} $$ are computed via score function (e.g., dot, additive, etc.)             | Allows decoder to focus on different parts of the input    | More computationally expensive, but powerful                    |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### üí° Key Takeaway\n",
    "\n",
    "- **Without attention**, the model summarizes the entire sequence in the last hidden state: a bottleneck for long sequences.\n",
    "- **With attention**, the model uses **all** hidden states, reweighted per decoding step, enabling more **flexible and accurate decoding**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fae4c5f4-7f33-41d0-9aad-89f727dd6f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 1.3734\n",
      "Epoch 2 loss: 1.3705\n",
      "Epoch 3 loss: 1.3689\n",
      "Epoch 4 loss: 1.3691\n",
      "Epoch 5 loss: 1.3679\n",
      "Epoch 6 loss: 1.3646\n",
      "Epoch 7 loss: 1.3575\n",
      "Epoch 8 loss: 1.3573\n",
      "Epoch 9 loss: 1.3543\n",
      "Epoch 10 loss: 1.3532\n",
      "Test accuracy: 0.2935\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset (subset for speed)\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "newsgroups = fetch_20newsgroups(subset='all', categories=categories, remove=('headers','footers','quotes'))\n",
    "\n",
    "# Simple tokenizer & vocabulary using CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=1000, stop_words='english')\n",
    "X_counts = vectorizer.fit_transform(newsgroups.data)\n",
    "\n",
    "# Convert to dense and truncate/pad sequences to fixed length (e.g., 100 words)\n",
    "X = X_counts.toarray()\n",
    "max_len = 100\n",
    "X = np.array([np.pad(x[:max_len], (0, max_len - len(x[:max_len])), 'constant') for x in X])\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(newsgroups.target)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Dataset wrapper\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.LongTensor(X)\n",
    "        self.y = torch.LongTensor(y)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_ds = NewsDataset(X_train, y_train)\n",
    "test_ds = NewsDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=64)\n",
    "\n",
    "# Define simple RNN with Attention\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hidden_dim, 1)\n",
    "    def forward(self, encoder_outputs):\n",
    "        # encoder_outputs: (batch_size, seq_len, hidden_dim)\n",
    "        weights = torch.softmax(self.attn(encoder_outputs).squeeze(-1), dim=1)  # (batch_size, seq_len)\n",
    "        weighted = torch.bmm(weights.unsqueeze(1), encoder_outputs).squeeze(1)  # (batch_size, hidden_dim)\n",
    "        return weighted, weights\n",
    "\n",
    "class RNNWithAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.rnn = nn.GRU(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.attention = Attention(hidden_dim * 2)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        outputs, _ = self.rnn(emb)\n",
    "        attn_output, attn_weights = self.attention(outputs)\n",
    "        logits = self.fc(attn_output)\n",
    "        return logits, attn_weights\n",
    "\n",
    "# Instantiate model\n",
    "vocab_size = 1000\n",
    "embed_dim = 64\n",
    "hidden_dim = 128\n",
    "output_dim = len(categories)\n",
    "\n",
    "model = RNNWithAttention(vocab_size, embed_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop (just a few epochs for demo)\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        logits, attn = model(batch_x)\n",
    "        loss = criterion(logits, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Quick evaluation on test set accuracy\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        logits, _ = model(batch_x)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == batch_y).sum().item()\n",
    "        total += batch_y.size(0)\n",
    "print(f\"Test accuracy: {correct / total:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c324095a-2fc4-4bb3-b5b5-863209725c3c",
   "metadata": {},
   "source": [
    "## üîç Multi-Head Attention \n",
    "\n",
    "Multi-Head Attention allows the model to jointly attend to information from different representation subspaces at different positions. Instead of performing a single attention function, it runs multiple attention mechanisms (\"heads\") in parallel.\n",
    "\n",
    "üß† Analogy\n",
    "- Think of multi-head attention like having a committee of experts:\n",
    "\n",
    "- Each expert (head) looks at the same document (input),\n",
    "\n",
    "- But each uses a different lens or perspective (different projection),\n",
    "\n",
    "- Then their opinions are combined (concatenated) and merged (via $W^O$) into a final decision.\n",
    "\n",
    "### 1. Inputs\n",
    "\n",
    "Let:\n",
    "\n",
    "- $Q \\in \\mathbb{R}^{T_q \\times d_{\\text{model}}}$: Query matrix (e.g., decoder hidden states or input embeddings)  \n",
    "- $K \\in \\mathbb{R}^{T_k \\times d_{\\text{model}}}$: Key matrix (e.g., encoder hidden states or input embeddings)  \n",
    "- $V \\in \\mathbb{R}^{T_v \\times d_{\\text{model}}}$: Value matrix (typically same shape as $K$)  \n",
    "\n",
    "where $T_q$, $T_k$, and $T_v$ are the sequence lengths for the query, key, and value, and $d_{\\text{model}}$ is the model dimension.\n",
    "\n",
    "### 2. Linear Projections for Each Head\n",
    "\n",
    "For each of the $h$ attention heads, we project $Q$, $K$, and $V$ using different learned linear transformations:\n",
    "\n",
    "$$\n",
    "Q_i = Q W_i^Q,\\quad K_i = K W_i^K,\\quad V_i = V W_i^V\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $W_i^Q, W_i^K, W_i^V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$  \n",
    "- Typically, $d_k = d_v = d_{\\text{model}} / h$\n",
    "\n",
    "### 3. Scaled Dot-Product Attention (per head)\n",
    "\n",
    "Each head computes attention as:\n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(Q_i, K_i, V_i) = \\text{softmax}\\left( \\frac{Q_i K_i^\\top}{\\sqrt{d_k}} \\right) V_i\n",
    "$$\n",
    "\n",
    "This results in a matrix of shape $(T_q \\times d_v)$ for each head.\n",
    "\n",
    "### 4. Concatenate and Final Linear Projection\n",
    "\n",
    "After computing all $h$ heads, we concatenate them along the feature dimension and apply a final linear projection:\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h) W^O\n",
    "$$\n",
    "\n",
    "where $W^O \\in \\mathbb{R}^{h d_v \\times d_{\\text{model}}}$ is a learned output projection matrix.\n",
    "\n",
    "### 5. Shape Summary\n",
    "\n",
    "Assuming:\n",
    "\n",
    "- $d_{\\text{model}} = 512$  \n",
    "- $h = 8$ (so $d_k = d_v = 64$)\n",
    "\n",
    "Then:\n",
    "\n",
    "- Each head output: $(T_q \\times d_k)$  \n",
    "- Concatenated heads: $(T_q \\times (h \\cdot d_v)) = (T_q \\times 512)$  \n",
    "- Final output: $(T_q \\times d_{\\text{model}})$ after applying $W^O$\n",
    "\n",
    "---\n",
    "\n",
    "This formulation enables the model to learn attention patterns from multiple perspectives simultaneously, improving its ability to model complex dependencies.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61167711-09d1-4b04-803c-74338353271a",
   "metadata": {},
   "source": [
    "| Feature                          | Single-Head Attention                            | Multi-Head Attention                                           |\n",
    "|----------------------------------|--------------------------------------------------|----------------------------------------------------------------|\n",
    "| **Definition**                   | One attention mechanism                         | Multiple attention mechanisms in parallel                     |\n",
    "| **Number of Context Vectors**    | One per query                                   | One per head, then concatenated into one                      |\n",
    "| **Computation**                  | Uses single Q, K, V projections                 | Splits Q, K, V into `h` parts, each processed independently   |\n",
    "| **Projection Dimensions**        | Q, K, V: $\\mathbb{R}^{d_{\\text{model}}}$        | Q, K, V: $\\mathbb{R}^{d_k = d_{\\text{model}}/h}$ per head     |\n",
    "| **Attention Diversity**          | Limited to one representation                   | Can attend to information from different representation subspaces |\n",
    "| **Output Shape**                 | $\\mathbb{R}^{T_q \\times d_{\\text{model}}}$      | $\\mathbb{R}^{T_q \\times d_{\\text{model}}}$ (after concat + projection) |\n",
    "| **Learned Parameters**           | One set of $W^Q$, $W^K$, $W^V$, $W^O$            | One set per head: $W_i^Q$, $W_i^K$, $W_i^V$, shared $W^O$     |\n",
    "| **Expressive Power**            | Lower                                            | Higher, captures multiple relationships                       |\n",
    "| **Used in Transformer?**         | No                                              | Yes (standard in Transformers)                                |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98bffdac-5651-46a7-abb3-6b29c04cde2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 1.3704\n",
      "Epoch 2 loss: 1.3652\n",
      "Epoch 3 loss: 1.3617\n",
      "Epoch 4 loss: 1.3655\n",
      "Epoch 5 loss: 1.3594\n",
      "Epoch 6 loss: 1.3579\n",
      "Epoch 7 loss: 1.3628\n",
      "Epoch 8 loss: 1.3587\n",
      "Epoch 9 loss: 1.3580\n",
      "Epoch 10 loss: 1.3564\n",
      "Test accuracy: 0.2965\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load dataset (subset for speed)\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "newsgroups = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# 2. Vectorize text using CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=10000, stop_words='english')\n",
    "X_counts = vectorizer.fit_transform(newsgroups.data)\n",
    "\n",
    "# 3. Convert to dense array and pad/truncate to fixed length\n",
    "X = X_counts.toarray()\n",
    "max_len = 100\n",
    "X = np.array([np.pad(x[:max_len], (0, max_len - len(x[:max_len])), 'constant') for x in X])\n",
    "\n",
    "# 4. Encode labels\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(newsgroups.target)\n",
    "\n",
    "# 5. Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 6. Dataset wrapper\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.LongTensor(X)\n",
    "        self.y = torch.LongTensor(y)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_ds = NewsDataset(X_train, y_train)\n",
    "test_ds = NewsDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=64)\n",
    "\n",
    "# 7. Define model with RNN + Multihead Attention\n",
    "class RNNWithMultiheadAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.rnn = nn.GRU(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=hidden_dim*2, num_heads=num_heads, batch_first=False)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)  # (batch_size, seq_len, embed_dim)\n",
    "        rnn_out, _ = self.rnn(emb)  # (batch_size, seq_len, hidden_dim*2)\n",
    "        rnn_out = rnn_out.transpose(0, 1)  # (seq_len, batch_size, hidden_dim*2)\n",
    "        attn_output, _ = self.attn(rnn_out, rnn_out, rnn_out)  # self-attention\n",
    "        attn_output_mean = attn_output.mean(dim=0)  # (batch_size, hidden_dim*2)\n",
    "        logits = self.fc(attn_output_mean)  # (batch_size, output_dim)\n",
    "        return logits, None\n",
    "\n",
    "# 8. Instantiate model\n",
    "vocab_size = 1000\n",
    "embed_dim = 64\n",
    "hidden_dim = 128\n",
    "output_dim = len(categories)\n",
    "model = RNNWithMultiheadAttention(vocab_size, embed_dim, hidden_dim, output_dim, num_heads=4)\n",
    "\n",
    "# 9. Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 10. Training loop\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model(batch_x)\n",
    "        loss = criterion(logits, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# 11. Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        logits, _ = model(batch_x)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == batch_y).sum().item()\n",
    "        total += batch_y.size(0)\n",
    "print(f\"Test accuracy: {correct / total:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc12bff-8e21-4670-9e2e-4e19c9a099e6",
   "metadata": {},
   "source": [
    "# ü§ñ What Are Transformers?\n",
    "\n",
    "Transformers are a **type of deep learning model architecture** that has revolutionized **natural language processing (NLP)** and is now used in **vision**, **speech**, and **time series forecasting**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîß What Is a Transformer?\n",
    "\n",
    "The **Transformer** was introduced in the paper [‚ÄúAttention Is All You Need‚Äù (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762). Unlike previous sequence models like RNNs or LSTMs, Transformers **do not process data sequentially**. Instead, they operate **in parallel** using a mechanism called **self-attention**.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Key Concepts\n",
    "\n",
    "| Component              | Description |\n",
    "|------------------------|-------------|\n",
    "| **Self-Attention**     | Lets each word \"attend\" to others to understand context. Example: In ‚ÄúShe fed her dog because it was hungry,‚Äù the word ‚Äúit‚Äù refers to ‚Äúdog.‚Äù |\n",
    "| **Positional Encoding**| Adds order information since Transformers process all words at once. |\n",
    "| **Multi-Head Attention**| Uses multiple attention mechanisms in parallel to learn different relationships between words. |\n",
    "| **Feedforward Layers** | Standard fully connected (dense) layers applied after attention. |\n",
    "| **Layer Norm & Residuals** | Help stabilize training and keep gradient flow healthy. |\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ Transformer vs RNN\n",
    "\n",
    "| Feature             | RNN/LSTM                | Transformer              |\n",
    "|---------------------|-------------------------|--------------------------|\n",
    "| **Sequential**      | Yes                     | No (parallelized)        |\n",
    "| **Long-range context**| Hard to learn          | Easy with attention      |\n",
    "| **Training Speed**  | Slower                  | Faster                   |\n",
    "| **Performance (NLP)**| Good                   | State-of-the-art         |\n",
    "\n",
    "---\n",
    "\n",
    "# üß† Self-Attention Mechanism (Transformers)\n",
    "\n",
    "## üîç Purpose\n",
    "\n",
    "Self-attention allows each token in a sequence to **attend to all other tokens**, capturing contextual dependencies regardless of their position. This is especially powerful in language tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## üß± Core Components\n",
    "\n",
    "Given a sequence of token embeddings $X \\in \\mathbb{R}^{n \\times d}$:\n",
    "\n",
    "- $n$: Sequence length  \n",
    "- $d$: Embedding dimension  \n",
    "\n",
    "We compute:\n",
    "\n",
    "- **Query** matrix: $Q = XW^Q$  \n",
    "- **Key** matrix: $K = XW^K$  \n",
    "- **Value** matrix: $V = XW^V$  \n",
    "\n",
    "Where:\n",
    "- $W^Q, W^K, W^V \\in \\mathbb{R}^{d \\times d_k}$ are learnable weight matrices  \n",
    "- $d_k$: Dimension of queries and keys (often $d_k = d / h$ if using multi-head attention)\n",
    "\n",
    "---\n",
    "\n",
    "## üìê Scaled Dot-Product Attention\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax} \\left( \\frac{QK^\\top}{\\sqrt{d_k}} \\right)V\n",
    "$$\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "1. **Similarity**: Compute $QK^\\top$, the dot product between queries and keys.\n",
    "2. **Scaling**: Divide by $\\sqrt{d_k}$ to stabilize gradients.\n",
    "3. **Softmax**: Converts scores to probabilities.\n",
    "4. **Weighting**: Applies those probabilities to the value vectors $V$.\n",
    "\n",
    "The result is a **contextualized representation** of the input sequence.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è What Happens for a Token $x_i$\n",
    "\n",
    "1. Create query vector $q_i$ from $x_i$\n",
    "2. Compare $q_i$ with all key vectors $k_j$\n",
    "3. Compute attention weights via softmax\n",
    "4. Use weights to combine value vectors $v_j$\n",
    "5. Output is a context-aware vector for token $x_i$\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Why Is This Important?\n",
    "\n",
    "- **Parallelizable**: Unlike RNNs/LSTMs, the whole sequence is processed at once.\n",
    "- **Global context**: Each token attends to every other token.\n",
    "- **Dynamic**: Attention changes based on input, allowing flexible modeling.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå In Transformers\n",
    "\n",
    "Self-attention is a core operation used repeatedly in the Transformer architecture ‚Äî in both the encoder and decoder. It's often used in **Multi-Head Attention**, where several self-attention heads run in parallel to learn richer relationships.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f3cf7b-9e41-4a4b-a461-0ce36aa4141c",
   "metadata": {},
   "source": [
    "| Feature                     | Attention                                         | Self-Attention                                         |\n",
    "|----------------------------|--------------------------------------------------|--------------------------------------------------------|\n",
    "| **Definition**             | A mechanism that lets the model focus on relevant parts of a **different sequence**. | A mechanism that lets each element of a sequence attend to **other elements within the same sequence**. |\n",
    "| **Typical Use Case**       | Decoder attends to encoder output (e.g., translation). | Encoder or decoder attends to its own inputs (e.g., understanding sentence structure). |\n",
    "| **Input Source**           | Query (Q), Key (K), and Value (V) from **different sequences**. | Query (Q), Key (K), and Value (V) from **the same sequence**. |\n",
    "| **Formula**                | $Attention(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V$ | Same formula, but $Q = K = V$ from the same sequence. |\n",
    "| **Model Type Examples**    | Seq2Seq with attention, Encoder-Decoder models.   | Transformers, BERT, GPT, etc.                          |\n",
    "| **Purpose**                | Helps decoder know which encoder outputs to focus on. | Helps the model understand relationships within a single input (e.g., long-distance dependencies in a sentence). |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88044b13-0b70-4b3b-a75f-b94f051914c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.4609\n",
      "Epoch 2, Loss: 1.3851\n",
      "Epoch 3, Loss: 1.3848\n",
      "Epoch 4, Loss: 1.3693\n",
      "Epoch 5, Loss: 1.3750\n",
      "Epoch 6, Loss: 1.3686\n",
      "Epoch 7, Loss: 1.3564\n",
      "Epoch 8, Loss: 1.3552\n",
      "Epoch 9, Loss: 1.3464\n",
      "Epoch 10, Loss: 1.3418\n",
      "Epoch 11, Loss: 1.3419\n",
      "Epoch 12, Loss: 1.3284\n",
      "Epoch 13, Loss: 1.3265\n",
      "Epoch 14, Loss: 1.3181\n",
      "Epoch 15, Loss: 1.3179\n",
      "Epoch 16, Loss: 1.3146\n",
      "Epoch 17, Loss: 1.3141\n",
      "Epoch 18, Loss: 1.3038\n",
      "Epoch 19, Loss: 1.3008\n",
      "Epoch 20, Loss: 1.3101\n",
      "Epoch 21, Loss: 1.3018\n",
      "Epoch 22, Loss: 1.3001\n",
      "Epoch 23, Loss: 1.2988\n",
      "Epoch 24, Loss: 1.2989\n",
      "Epoch 25, Loss: 1.2967\n",
      "Epoch 26, Loss: 1.2950\n",
      "Epoch 27, Loss: 1.2979\n",
      "Epoch 28, Loss: 1.2931\n",
      "Epoch 29, Loss: 1.2948\n",
      "Epoch 30, Loss: 1.2943\n",
      "Test accuracy: 0.3083\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Transformer-based classifier\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_classes, max_len):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len, embed_dim))\n",
    "        \n",
    "        self.transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.transformer_encoder_layer, num_layers=2)\n",
    "        \n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len)\n",
    "        emb = self.embedding(x) + self.pos_embedding[:, :x.size(1), :]  # (batch, seq, embed)\n",
    "        enc_output = self.transformer_encoder(emb)  # (batch, seq, embed)\n",
    "        pooled = enc_output.mean(dim=1)  # global average pooling\n",
    "        return self.classifier(pooled)\n",
    "\n",
    "# Parameters\n",
    "vocab_size = 10000\n",
    "embed_dim = 128\n",
    "num_heads = 4\n",
    "num_classes = len(np.unique(y))\n",
    "max_len = 100\n",
    "\n",
    "model = TransformerClassifier(vocab_size, embed_dim, num_heads, num_classes, max_len)\n",
    "\n",
    "# Training setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(30):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch_x)\n",
    "        loss = criterion(logits, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        logits = model(batch_x)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == batch_y).sum().item()\n",
    "        total += batch_y.size(0)\n",
    "print(f\"Test accuracy: {correct / total:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e32672-5452-411d-9693-d9cd53d1abcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-macos)",
   "language": "python",
   "name": "tf-macos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
